server:
  port: 17928

spring:
  jackson:
    time-zone: GMT+8
    date-format: yyyy-MM-dd HH:mm:ss

spring.datas

#  shardingsphere:
#    datasource:
#      names: ds
#      ds:
#        driver-class-name: com.mysql.cj.jdbc.Driver
#        url: jdbc:mysql://124.222.96.147:3306/vegetables?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai
#        username: vegetables
#        password: vegetables0928..
#        type: com.alibaba.druid.pool.DruidDataSource
#    sharding:
#      tables:
#        test:
#          actual-data-nodes: ds.test
#          table-strategy:
#            complex:
#              sharding-columns: bill_start_time,corp_id
#              precise-algorithm-class-name: com.example.vegetables.sharding.DateShardingAlgorithm
#              range-algorithm-class-name: com.example.vegetables.sharding.DateShardingAlgorithm
#              algorithm-class-name: com.example.vegetables.sharding.DateShardingAlgorithm
#            hint:
#              algorithm-class-name: com.example.vegetables.sharding.DateShardingAlgorithm
#        prod:
#          actual-data-nodes: ds.prod
#          table-strategy:
#            complex:
#              sharding-columns: bill_start_time,corp_id
#              algorithm-class-name: com.example.vegetables.sharding.DateShardingAlgorithm
#      binding-tables: test,prod
#      defaultDataSourceName: ds
#    props:
#      sql:
#        show: false


  redis:
    port: 6379
    host: 124.222.96.147
    database: 0
    timeout: 5000
    password: whatever

#  kafka:
#    bootstrap-servers: 124.222.96.147:9092
#    producer: # 生产者
#      retries: 3 # 设置大于0的值，则客户端会将发送失败的记录重新发送
#      batch-size: 16384
#      buffer-memory: 33554432
#      acks: 1
      # 指定消息key和消息体的编解码方式
#      key-serializer: org.apache.kafka.common.serialization.StringSerializer
#      value-serializer: org.apache.kafka.common.serialization.StringSerializer
#    consumer:
#      group-id: default-group
#      enable-auto-commit: false
#      auto-offset-reset: earliest
#      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
#      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
#    listener:
      # 当每一条记录被消费者监听器（ListenerConsumer）处理之后提交
      # RECORD
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交
      # BATCH
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交
      # TIME
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交
      # COUNT
      # TIME |　COUNT　有一个条件满足时提交
      # COUNT_TIME
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交
      # MANUAL
      # 手动调用Acknowledgment.acknowledge()后立即提交，一般使用这种
      # MANUAL_IMMEDIATE
#      ack-mode: manual_immediate

#  main:
#    allow-bean-definition-overriding: true

mybatis-plus:
  configuration:
    log-impl: com.example.vegetables.config.LogStdOutImpl
  mapper-locations: classpath:mapper/*.xml
  global-config:
    db-config:
      logic-not-delete-value: 0
      logic-delete-value: 1

# 日志配置
logging:
  level:
    com.example.vegetables: debug
    org.springframework: warn

